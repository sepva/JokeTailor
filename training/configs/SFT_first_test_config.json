{
    "training_class": "SFT",
    "training_args": {
        "model_id": "prithivMLmods/Bellatrix-Tiny-0.5B",
        "dataset_id": "SeppeV/JokeTailor_big_set_annotated",
        "percentage_data": 100,
        "trainer_config": {
            "hub_model_id": "SFT_joketailor_bellatrix",
            "output_dir": "/scratch/leuven/368/vsc36814",
            "report_to": "wandb",
            "run_name": "test_SFT_setup",
            "per_device_train_batch_size": 10,
            "per_device_eval_batch_size": 10,
            "save_total_limit": 2,
            "eval_strategy": "epoch",
            "push_to_hub": true
        },
        "lora_config": {
            "lora_alpha": 16,
            "lora_dropout": 0.1,
            "r": 64,
            "bias": "none",
            "task_type": "CAUSAL_LM"
        },
        "prompt": "Generate a joke:"
    }
}
