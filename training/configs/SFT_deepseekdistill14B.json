{
    "training_class": "SFT",
    "training_args": {
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "dataset_id": "SeppeV/JokeTailor_big_set_annotated",
        "percentage_data": 100,
        "trainer_config": {
            "hub_model_id": "SFT_joketailor_qwen14",
            "output_dir": "/scratch/leuven/368/vsc36814",
            "report_to": "wandb",
            "run_name": "SFTqwen1.5",
            "per_device_train_batch_size": 8,
            "per_device_eval_batch_size": 8,
            "save_total_limit": 2,
            "eval_strategy": "epoch",
            "push_to_hub": true
        },
        "lora_config": {
            "lora_alpha": 16,
            "lora_dropout": 0.1,
            "r": 64,
            "bias": "none",
            "task_type": "CAUSAL_LM"
        },
        "bnb_config": {
            "load_in_8bit": true,
            "load_in_4bit": false
        },
        "prompt": "Generate a joke:"
    }
}